---
title: "COMPSCIX 415.2 Homework 7"
author: "Chi Nguyen"
date: "3/16/2019"
output:
  html_document:
    self_contained: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

My Github repository for my assignments can be found at this URL: https://github.com/chimandy/compscix-415-2-assignments.git

```{r}
library(broom)
library(tidyverse)
library(ggplot2)
```


###*Exercise 1*

Load the train.csv dataset into R. How many observations and columns are there?
```{r}
file_path<-'train.csv'
train_data<- read_csv(file='train.csv')
```
```{r}
df <- read.table("train.csv", header=TRUE, 
   sep=",")
glimpse(df)
```


There are 1,460 observations and 81 variables. 

###*Excercise 2*

Normally at this point you would spend a few days on EDA, but for this homework we will do some very basic EDA and get right to fitting some linear regression models.

Our target will be SalePrice.

1. Visualize the distribution of SalePrice.


```{r}
train_data%>% ggplot()+
    geom_histogram(aes(x =SalePrice))
```

2. Visualize the covariation between SalePrice and Neighborhood.



```{r}
c <- cov(df$SalePrice, as.numeric(df$Neighborhood))
c
```

```{r}

ggplot(data = train_data, mapping = aes(x = SalePrice, y=Neighborhood))+
geom_point()+
coord_flip()

```


3. Visualize the covariation between SalePrice and OverallQual.


```{r}
c2 <- cov(df$SalePrice, as.numeric(df$OverallQual))
c2
```



```{r}
train_data%>% ggplot (aes(x = SalePrice, y = OverallQual, fill=Neighborhood))+
 geom_boxplot(position='dodge')+
 coord_flip()

```



###*Exercise 3*
Our target is called SalePrice. First, we can fit a simple regression model consisting of only the intercept (the average of SalePrice). Fit the model and then use the broom package to take a look at the coefficient,compare the coefficient to the average value of SalePrice, and take a look at the R-squared.

Avarage Sale Price 
```{r}
mean_SalePrice<-mean(df$SalePrice)
sd_SalePrice <-sd(df$SalePrice)
mean_SalePrice
sd_SalePrice
```



```{r}
train_data_1<-train_data%>%mutate(mean_SalePrice)
(train_lm <- lm(formula =SalePrice ~Id, data = train_data_1))

```

```{r}
tidy(train_lm)
```

```{r}
summary(train_lm)

```

```{r}
glance(train_lm)
```


The coefficient from lm model which is about 183,937 is quite different to the avarage value of SalePrice which is 180921.2. The R square is low close to zero to represent week relationship.

###*Exercise 4*
Now fit a linear regression model using GrLivArea, OverallQual, and Neighborhood as the features. Don’t forget to look at data_description.txt to understand what these variables mean. Ask yourself these questions before fitting the model:

What kind of relationship will these features have with our target?
Can the relationship be estimated linearly?
Are these good features, given the problem we are trying to solve?
After fitting the model, output the coefficients and the R-squared using the broom package.




```{r}
train_data_2<-train_data_1%>%mutate(Neighborhood_fct=factor(Neighborhood, ordered=FALSE))
train_mult_lm <- lm(SalePrice ~ GrLivArea + OverallQual + Neighborhood, data = train_data_2)
tidy(train_mult_lm)
```
```{r}
glance(train_mult_lm)
```

Coefficient= -34829.2399
R square = 0.7868484
Answer these questions:

*How would you interpret the coefficients on GrLivArea and OverallQual?*

OverallQual: Rates the overall material and finish of the house
GrLivArea: Above grade (ground) living area square feet

SalePrice is directly impacted by the value of OverallQual and GrLivArea based on the value of each coefficient of each feature in assumption that all other features are held constant. 

*How would you interpret the coefficient on NeighborhoodBrkSide?*
Neighborhood: Physical locations within Ames city limits
The negative  value if coeeficient states that if the neighborhood location is at NeighborhoodBrkSide, the SalePrice will be devalued. 


*Are the features significant?*

The P value of these features are close to zero so these features are significant
*Are the features practically significant?*

*This is a question that depends on the data, and your knowledge of it.*

It is also very pratical significant if we try to estimate the value of the house based on these siginificant factors.


*Is the model a good fit?*
since we have larger R^2 when we add more features, the model fits the data better

###*Excercise 6*

One downside of the linear model is that it is sensitive to unusual values because the distance incorporates a squared term. Fit a linear model to the simulated data below (use y as the target and x as the feature), and look at the resulting coefficients and R-squared. Rerun it about 5-6 times to generate different simulated datasets. What do you notice about the model’s coefficient on x and the R-squared values?

```{r}
sim1a <- tibble(
  x = rep(1:10, each = 3),
  y = x * 1.5 + 6 + rt(length(x), df = 2)
)
as.tibble(sim1a)
```

```{r}
sim1a_lm <- lm(formula =y ~x, data = sim1a)
tidy(sim1a_lm)
```

```{r}
glance(sim1a_lm)
```

After several attemps of running the model, it shows that the values of coefficient and R square keeps changing each time. However, R square change is so much more than coffecient. It states that the model will look most likely the same but the variance will varies after each run. 
